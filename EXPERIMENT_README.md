# ğŸš€ Experimentos de Reinforcement Learning - MÃ©tricas TensorBoard

Este projeto implementa um sistema completo de monitoramento e anÃ¡lise para experimentos de Reinforcement Learning usando DQN e A3C no ambiente LunarLander-v3.

## ğŸ“Š MÃ©tricas Implementadas

### DQN (Deep Q-Network)
- **Loss da Rede Neural**: Perda MSE entre Q-values preditos e target
- **Recompensa por EpisÃ³dio**: Performance do agente
- **DuraÃ§Ã£o do EpisÃ³dio**: NÃºmero de passos atÃ© terminar
- **Epsilon**: Valor atual da exploraÃ§Ã£o epsilon-greedy
- **Recompensa MÃ©dia (100 episÃ³dios)**: TendÃªncia de performance

### A3C (Asynchronous Advantage Actor-Critic)
- **Policy Loss**: Perda do ator (incentiva aÃ§Ãµes vantajosas)
- **Value Loss**: Perda do crÃ­tico (erro na prediÃ§Ã£o V(s))
- **Entropy Loss**: Medida de exploraÃ§Ã£o da polÃ­tica
- **Advantage**: Indicativo de qualidade das aÃ§Ãµes
- **Explained Variance**: Qualidade das prediÃ§Ãµes do crÃ­tico
- **Recompensa por EpisÃ³dio**: Performance do agente
- **DuraÃ§Ã£o do EpisÃ³dio**: NÃºmero de passos atÃ© terminar

## ğŸ”§ ConfiguraÃ§Ãµes Experimentais

### Experimentos DQN:
1. **Baseline**: ConfiguraÃ§Ã£o padrÃ£o (`dqn_config.yaml`)
2. **Learning Rate Baixo**: LR = 0.0001 (`dqn_config_lr_low.yaml`)
3. **Learning Rate Alto**: LR = 0.01 (`dqn_config_lr_high.yaml`)
4. **Gamma Baixo**: Î³ = 0.95 (`dqn_config_gamma_low.yaml`)
5. **Rede Simples**: [64] neurÃ´nios (`dqn_config_simple_net.yaml`)

### Experimentos A3C:
1. **Baseline**: ConfiguraÃ§Ã£o padrÃ£o (`a3c_config.yaml`)
2. **Learning Rate Baixo**: LR = 0.00001 (`a3c_config_lr_low.yaml`)
3. **Entropy Zero**: entropy_coef = 0.0 (`a3c_config_entropy_zero.yaml`)

## ğŸš€ Como Executar

### Treinamento Individual

```bash
# DQN com configuraÃ§Ã£o padrÃ£o
cd scripts
python train.py --agent dqn

# A3C com configuraÃ§Ã£o padrÃ£o  
python train.py --agent a3c
```

### Executar Todos os Experimentos

```bash
cd scripts
python run_experiments.py
```

### Visualizar Resultados no TensorBoard

```bash
# Visualizar todos os experimentos
tensorboard --logdir results/logs

# Visualizar experimento especÃ­fico
tensorboard --logdir results/logs/DQN_lr_0.01
```

### AnÃ¡lise Automatizada

```bash
cd scripts
python analyze_results.py
```

## ğŸ“ˆ InterpretaÃ§Ã£o das MÃ©tricas

### Loss (DQN)
- **Diminuindo**: âœ“ Rede estÃ¡ convergindo
- **Oscilando**: âš ï¸ PossÃ­vel instabilidade na taxa de aprendizado
- **Aumentando**: âœ— Problema na configuraÃ§Ã£o

### Policy Loss (A3C)
- **Negativa**: Normal (maximizaÃ§Ã£o da vantagem)
- **Diminuindo em mÃ³dulo**: âœ“ PolÃ­tica melhorando
- **Muito instÃ¡vel**: âš ï¸ LR muito alto

### Value Loss (A3C)
- **Diminuindo**: âœ“ CrÃ­tico aprendendo a predizer retornos
- **Estagnada**: âš ï¸ CrÃ­tico nÃ£o estÃ¡ aprendendo

### Entropy Loss (A3C)
- **Negativa**: Normal (entropia Ã© positiva, loss Ã© negativa)
- **PrÃ³xima de zero**: âš ï¸ Baixa exploraÃ§Ã£o
- **Muito negativa**: âœ“ Alta exploraÃ§Ã£o

### Explained Variance (A3C)
- **PrÃ³xima de 1**: âœ“ CrÃ­tico prediz bem os retornos
- **PrÃ³xima de 0**: âš ï¸ CrÃ­tico nÃ£o estÃ¡ aprendendo
- **Negativa**: âœ— CrÃ­tico pior que baseline

## ğŸ” AnÃ¡lise Comparativa

### TensorBoard - ComparaÃ§Ã£o de Experimentos:
1. Abra o TensorBoard: `tensorboard --logdir results/logs`
2. Use as abas para comparar mÃ©tricas
3. Selecione mÃºltiplos experimentos para sobreposiÃ§Ã£o
4. Analise correlaÃ§Ãµes entre mÃ©tricas

### Perguntas-Chave para AnÃ¡lise:

1. **A loss estÃ¡ convergindo?** Observe se diminui consistentemente
2. **Existe correlaÃ§Ã£o entre loss e recompensa?** Loss estÃ¡vel â†’ recompensa estÃ¡vel?
3. **O epsilon estÃ¡ decaindo adequadamente?** (DQN)
4. **A entropia estÃ¡ adequada para exploraÃ§Ã£o?** (A3C)
5. **O crÃ­tico estÃ¡ aprendendo?** Explained variance melhorando? (A3C)

## ğŸ“Š Estrutura dos Resultados

```
results/
â”œâ”€â”€ logs/                     # Logs do TensorBoard
â”‚   â”œâ”€â”€ DQN/                 # Baseline DQN
â”‚   â”œâ”€â”€ DQN_lr_0.01/         # DQN LR alto
â”‚   â”œâ”€â”€ DQN_lr_0.0001/       # DQN LR baixo
â”‚   â”œâ”€â”€ A3C/                 # Baseline A3C
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                   # Modelos salvos
â””â”€â”€ analysis/                 # AnÃ¡lises geradas
    â”œâ”€â”€ dqn_comparison.png
    â”œâ”€â”€ a3c_comparison.png
    â””â”€â”€ experiment_report.md
```

## ğŸ¯ Objetivos dos Experimentos

1. **Learning Rate**: Encontrar o equilÃ­brio entre velocidade e estabilidade
2. **Gamma**: Avaliar o impacto da "visÃ£o de futuro"
3. **Arquitetura**: Determinar complexidade adequada da rede
4. **Target Update Frequency**: Estabilidade do Q-learning (DQN)
5. **Entropy Coefficient**: Balanceamento exploraÃ§Ã£o vs. exploitation (A3C)

## ğŸ”¬ HipÃ³teses Testadas

- **LR Alto**: âš ï¸ Pode causar instabilidade e oscilaÃ§Ãµes
- **LR Baixo**: âš ï¸ ConvergÃªncia muito lenta
- **Gamma Baixo**: ğŸ¯ Foco em recompensas imediatas
- **Rede Simples**: âš¡ Pode ser suficiente para problemas simples
- **Entropy Zero**: ğŸš« Reduz exploraÃ§Ã£o drasticamente

## ğŸ“ RelatÃ³rio de AnÃ¡lise

ApÃ³s executar os experimentos, um relatÃ³rio detalhado Ã© gerado automaticamente em:
`results/analysis/experiment_report.md`

Este relatÃ³rio inclui:
- ComparaÃ§Ã£o de performance entre experimentos
- AnÃ¡lise das mÃ©tricas implementadas
- ConclusÃµes sobre os hiperparÃ¢metros testados
- RecomendaÃ§Ãµes para futuros experimentos

## ğŸ› ï¸ DependÃªncias

```
gymnasium[classic_control]
torch
tensorboard
numpy
matplotlib
pandas
```

## ğŸ“ PrÃ³ximos Passos

1. Implementar Prioritized Experience Replay (DQN)
2. Testar diferentes arquiteturas (LSTM, Attention)
3. Experimentos com diferentes ambientes
4. Implementar algoritmos mais avanÃ§ados (PPO, SAC)
5. Hyperparameter optimization automatizado