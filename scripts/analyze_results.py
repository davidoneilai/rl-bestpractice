"""
Script para an√°lise dos resultados dos experimentos
"""
import os
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import pandas as pd

def extract_tensorboard_data(log_dir, scalar_name):
    """Extrai dados de escalar do TensorBoard"""
    try:
        event_acc = EventAccumulator(log_dir)
        event_acc.Reload()
        
        scalar_events = event_acc.Scalars(scalar_name)
        steps = [event.step for event in scalar_events]
        values = [event.value for event in scalar_events]
        
        return steps, values
    except:
        return [], []

def analyze_experiment_results():
    """Analisa os resultados de todos os experimentos"""
    base_log_dir = "results/logs"
    
    # Configura√ß√µes dos experimentos
    experiments = {
        "DQN_baseline": "DQN",
        "DQN_lr_0.0001": "DQN - LR Baixo",
        "DQN_lr_0.01": "DQN - LR Alto", 
        "DQN_gamma_0.95": "DQN - Gamma Baixo",
        "DQN_simple_net": "DQN - Rede Simples",
        "A3C_baseline": "A3C",
        "A3C_lr_0.00001": "A3C - LR Baixo",
        "A3C_entropy_0": "A3C - Entropy Zero"
    }
    
    # M√©tricas para analisar
    dqn_metrics = ["Reward", "Loss", "Episode Duration", "Epsilon"]
    a3c_metrics = ["Global/Reward", "Global/Policy_Loss", "Global/Value_Loss", 
                   "Global/Entropy_Loss", "Global/Episode_Duration"]
    
    print("="*60)
    print("AN√ÅLISE DOS RESULTADOS DOS EXPERIMENTOS")
    print("="*60)
    
    # Analisar experimentos DQN
    print("\nü§ñ EXPERIMENTOS DQN:")
    print("-" * 40)
    
    dqn_results = {}
    for exp_name, exp_label in experiments.items():
        if "DQN" in exp_name:
            log_path = os.path.join(base_log_dir, exp_name.replace("_baseline", ""))
            if os.path.exists(log_path):
                print(f"\nüìä {exp_label}:")
                
                # Extrair dados de recompensa
                steps, rewards = extract_tensorboard_data(log_path, "Reward")
                if rewards:
                    dqn_results[exp_label] = {
                        'steps': steps,
                        'rewards': rewards,
                        'max_reward': max(rewards),
                        'final_avg_reward': np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards),
                        'convergence_episode': len(rewards)
                    }
                    
                    print(f"  ‚Ä¢ Recompensa m√°xima: {max(rewards):.2f}")
                    print(f"  ‚Ä¢ Recompensa m√©dia final (√∫ltimos 100): {np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards):.2f}")
                    print(f"  ‚Ä¢ Epis√≥dios treinados: {len(rewards)}")
                    
                    # Analisar loss se dispon√≠vel
                    _, losses = extract_tensorboard_data(log_path, "Loss")
                    if losses:
                        print(f"  ‚Ä¢ Loss final: {losses[-1]:.6f}")
                        print(f"  ‚Ä¢ Loss diminuiu: {'‚úì' if losses[-1] < losses[0] else '‚úó'}")
                else:
                    print("  ‚Ä¢ Sem dados dispon√≠veis")
    
    # Analisar experimentos A3C
    print("\nüîÑ EXPERIMENTOS A3C:")
    print("-" * 40)
    
    a3c_results = {}
    for exp_name, exp_label in experiments.items():
        if "A3C" in exp_name:
            log_path = os.path.join(base_log_dir, exp_name.replace("_baseline", ""))
            if os.path.exists(log_path):
                print(f"\nüìä {exp_label}:")
                
                # Extrair dados de recompensa
                steps, rewards = extract_tensorboard_data(log_path, "Global/Reward")
                if rewards:
                    a3c_results[exp_label] = {
                        'steps': steps,
                        'rewards': rewards,
                        'max_reward': max(rewards),
                        'final_avg_reward': np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)
                    }
                    
                    print(f"  ‚Ä¢ Recompensa m√°xima: {max(rewards):.2f}")
                    print(f"  ‚Ä¢ Recompensa m√©dia final: {np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards):.2f}")
                    print(f"  ‚Ä¢ Epis√≥dios treinados: {len(rewards)}")
                    
                    # Analisar m√©tricas espec√≠ficas do A3C
                    _, policy_loss = extract_tensorboard_data(log_path, "Global/Policy_Loss")
                    _, value_loss = extract_tensorboard_data(log_path, "Global/Value_Loss")
                    _, entropy_loss = extract_tensorboard_data(log_path, "Global/Entropy_Loss")
                    
                    if policy_loss:
                        print(f"  ‚Ä¢ Policy Loss final: {policy_loss[-1]:.6f}")
                    if value_loss:
                        print(f"  ‚Ä¢ Value Loss final: {value_loss[-1]:.6f}")
                    if entropy_loss:
                        print(f"  ‚Ä¢ Entropy Loss final: {entropy_loss[-1]:.6f}")
                else:
                    print("  ‚Ä¢ Sem dados dispon√≠veis")
    
    # Gerar gr√°ficos comparativos
    generate_comparison_plots(dqn_results, a3c_results)
    
    # Gerar relat√≥rio
    generate_report(dqn_results, a3c_results)

def generate_comparison_plots(dqn_results, a3c_results):
    """Gera gr√°ficos comparativos dos experimentos"""
    if not dqn_results and not a3c_results:
        print("\n‚ö†Ô∏è Nenhum dado dispon√≠vel para gerar gr√°ficos")
        return
    
    print("\nüìà Gerando gr√°ficos comparativos...")
    
    # Criar diret√≥rio para gr√°ficos
    os.makedirs("results/analysis", exist_ok=True)
    
    # Gr√°fico DQN
    if dqn_results:
        plt.figure(figsize=(12, 8))
        for exp_name, data in dqn_results.items():
            plt.plot(data['steps'], data['rewards'], label=exp_name, alpha=0.7)
        
        plt.title("Compara√ß√£o de Recompensas - Experimentos DQN")
        plt.xlabel("Epis√≥dios")
        plt.ylabel("Recompensa")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig("results/analysis/dqn_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    # Gr√°fico A3C
    if a3c_results:
        plt.figure(figsize=(12, 8))
        for exp_name, data in a3c_results.items():
            plt.plot(data['steps'], data['rewards'], label=exp_name, alpha=0.7)
        
        plt.title("Compara√ß√£o de Recompensas - Experimentos A3C")
        plt.xlabel("Epis√≥dios")
        plt.ylabel("Recompensa")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig("results/analysis/a3c_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    print("  ‚Ä¢ Gr√°ficos salvos em results/analysis/")

def generate_report(dqn_results, a3c_results):
    """Gera relat√≥rio detalhado dos experimentos"""
    print("\nüìù Gerando relat√≥rio detalhado...")
    
    report_path = "results/analysis/experiment_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Relat√≥rio de Experimentos - Reinforcement Learning\n\n")
        f.write("## Resumo Executivo\n\n")
        f.write("Este relat√≥rio apresenta os resultados dos experimentos realizados ")
        f.write("para avaliar o impacto de diferentes hiperpar√¢metros no desempenho ")
        f.write("dos algoritmos DQN e A3C no ambiente LunarLander-v3.\n\n")
        
        # Se√ß√£o DQN
        if dqn_results:
            f.write("## ü§ñ Experimentos DQN\n\n")
            f.write("| Experimento | Recompensa M√°xima | Recompensa Final | Epis√≥dios |\n")
            f.write("|-------------|-------------------|------------------|----------|\n")
            
            for exp_name, data in dqn_results.items():
                f.write(f"| {exp_name} | {data['max_reward']:.2f} | ")
                f.write(f"{data['final_avg_reward']:.2f} | {data['convergence_episode']} |\n")
            
            f.write("\n### An√°lise DQN:\n\n")
            f.write("**Observa√ß√µes sobre os experimentos DQN:**\n")
            f.write("- Varia√ß√µes na taxa de aprendizado mostram impacto significativo na converg√™ncia\n")
            f.write("- O fator de desconto (gamma) afeta a 'vis√£o de futuro' do agente\n")
            f.write("- A arquitetura da rede influencia a capacidade de aproxima√ß√£o\n\n")
        
        # Se√ß√£o A3C
        if a3c_results:
            f.write("## üîÑ Experimentos A3C\n\n")
            f.write("| Experimento | Recompensa M√°xima | Recompensa Final |\n")
            f.write("|-------------|-------------------|------------------|\n")
            
            for exp_name, data in a3c_results.items():
                f.write(f"| {exp_name} | {data['max_reward']:.2f} | ")
                f.write(f"{data['final_avg_reward']:.2f} |\n")
            
            f.write("\n### An√°lise A3C:\n\n")
            f.write("**Observa√ß√µes sobre os experimentos A3C:**\n")
            f.write("- O coeficiente de entropia √© crucial para a explora√ß√£o\n")
            f.write("- Taxa de aprendizado muito baixa pode levar a converg√™ncia lenta\n")
            f.write("- Equil√≠brio entre policy loss e value loss √© importante\n\n")
        
        f.write("## üìä M√©tricas Implementadas\n\n")
        f.write("### DQN:\n")
        f.write("- **Loss da Rede Neural**: Indica converg√™ncia do algoritmo\n")
        f.write("- **Dura√ß√£o do Epis√≥dio**: Tempo de sobreviv√™ncia/resolu√ß√£o\n")
        f.write("- **Epsilon**: Decaimento da explora√ß√£o\n\n")
        
        f.write("### A3C:\n")
        f.write("- **Policy Loss**: Perda do ator (actor)\n")
        f.write("- **Value Loss**: Perda do cr√≠tico\n")
        f.write("- **Entropy Loss**: Medida de explora√ß√£o\n")
        f.write("- **Advantage**: Indicativo de boas a√ß√µes\n")
        f.write("- **Explained Variance**: Qualidade das predi√ß√µes do cr√≠tico\n\n")
        
        f.write("## üîç Conclus√µes\n\n")
        f.write("1. **Taxa de Aprendizado**: Hiperpar√¢metro cr√≠tico - valores muito altos causam instabilidade\n")
        f.write("2. **Fator de Desconto**: Afeta significativamente a estrat√©gia do agente\n")
        f.write("3. **Arquitetura da Rede**: Redes mais complexas n√£o necessariamente s√£o melhores\n")
        f.write("4. **Explora√ß√£o**: Fundamental para evitar m√≠nimos locais\n\n")
        
        f.write("## üìà Visualiza√ß√µes\n\n")
        f.write("- Gr√°ficos comparativos dispon√≠veis em `results/analysis/`\n")
        f.write("- Use TensorBoard para an√°lise detalhada: `tensorboard --logdir results/logs`\n")
    
    print(f"  ‚Ä¢ Relat√≥rio salvo em {report_path}")

if __name__ == "__main__":
    analyze_experiment_results()